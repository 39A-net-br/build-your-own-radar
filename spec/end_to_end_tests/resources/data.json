[
  {
    "name": "Path-to-production mapping",
    "ring": "Adopt",
    "quadrant": "Techniques",
    "isNew": "TRUE",
    "description": "<p>Although <strong>path-to-production mapping</strong> has been a near-universal practice at Thoughtworks since codifying <em><a href=\"https://www.amazon.com/Continuous-Delivery-Deployment-Automation-Addison-Wesley/dp/0321601912\">Continuous Delivery</a></em>, we often come across organizations unfamiliar with the practice. The activity is most often done in a workshop with a cross-functional group of people —  that includes everyone involved in designing, developing, releasing and operating the software — around a shared whiteboard (or virtual equivalent). First, the steps in the process are listed in order, from the developer workstation all the way to production. Then, a facilitated session is used to capture further information and pain points. The most common technique we see is based on <a href=\"https://en.wikipedia.org/wiki/Value-stream_mapping\">value-stream mapping</a>, although plenty of <a href=\"https://caroli.org/en/path-to-production/\">process map</a> variants are equally valuable. The activity is often eye-opening for many of the participants, as they identify delays, risks and inconsistencies and continue to use the visual representation for the continuous improvement of the build and deploy process. We consider this technique so foundational that we were surprised to discover we hadn't blipped it before.</p>"
  },
  {
    "name": "Component visual regression testing",
    "ring": "Trial",
    "quadrant": "Techniques",
    "isNew": "FALSE",
    "description": "<p><a href=\"/radar/tools/visual-regression-testing-tools\">Visual regression testing</a> is a useful and powerful tool to have in your toolbox, but it has a significant cost given it's done for the entire page. With the rise of component-based frameworks such as <a href=\"/radar/languages-and-frameworks/react-js\">React</a> and <a href=\"/radar/languages-and-frameworks/vue-js\">Vue</a>, we've also seen the rise of <strong>component visual regression testing</strong>. This technique strikes a good balance between value and cost to ensure that no undesired visuals have been added to the application. In our experience, component visual regression testing presents fewer false positives and promotes a good architectural style. By using it with tools such as <a href=\"https://github.com/vitejs/vite\">Vite</a> and the webpack feature <a href=\"https://webpack.js.org/guides/hot-module-replacement/\">Hot Module Replacement (HMR)</a>, it could be seen as a paradigm shift for applying test-driven development to front-end development.</p>"
  },
  {
    "name": "Dragonfly",
    "ring": "Assess",
    "quadrant": "Platforms",
    "isNew": "TRUE",
    "description": "<p><strong><a href=\"https://github.com/dragonflydb/dragonfly\">Dragonfly</a></strong> is a new in-memory data store with compatible <a href=\"/radar/platforms/redis\">Redis</a> and Memcached APIs. It leverages the new Linux-specific <a href=\"https://github.com/axboe/liburing\">io_uring</a> API for I/O and implements <a href=\"https://dragonflydb.io/blog/2022/06/23/cache_design/\">novel algorithms and data structures</a> on top of a multithreaded, shared-nothing architecture. Because of these clever choices in implementation, Dragonfly achieves impressive results in performance. Although Redis continues to be our default choice for in-memory data store solutions, we do think Dragonfly is an interesting choice to assess.</p>"
  },
  {
    "name": "OrioleDB",
    "ring": "Hold",
    "quadrant": "Platforms",
    "isNew": "FALSE",
    "description": "<p><strong><a href=\"https://github.com/orioledb/orioledb/\">OrioleDB</a></strong> is a new storage engine for PostgreSQL. Our teams use PostgreSQL a lot, but its storage engine was originally designed for hard drives. Although there are several options to tune for modern hardware, it can be difficult and cumbersome to achieve optimal results. OrioleDB addresses these challenges by implementing a cloud-native storage engine with explicit support for solid-state drives (SSDs) and nonvolatile random-access memory (NVRAM). To try the new engine, first install the enhancement patches to the current <a href=\"https://www.postgresql.org/docs/current/tableam.html\">table access methods</a> and then install OrioleDB as a PostgreSQL extension. We believe OrioleDB has great potential to address several <a href=\"https://www.slideshare.net/AlexanderKorotkov/solving-postgresql-wicked-problems\">long-pending issues in PostgreSQL</a>, and we encourage you to carefully assess it.</p>"
  },
  {
    "name": "AWS Backup Vault Lock",
    "ring": "Trial",
    "quadrant": "Tools",
    "isNew": "TRUE",
    "description": "<p>When implementing robust, secure and reliable disaster recovery, it’s necessary to ensure that backups can't be deleted or altered before their expiry, either maliciously or accidentally. Previously, with AWS Backup, these policies and guarantees had to be implemented by hand. Recently, AWS has added the Vault Lock feature to ensure backups are immutable and untamperable. <a href=\"https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html\"><strong>AWS Backup Vault Lock</strong></a> enforces retention and deletion policies and prevents even those with administrator privileges from altering or deleting backup files. This has proved to be a valuable addition and fills a previously empty space.</p>"
  },
  {
    "name": "Databricks Overwatch",
    "ring": "Assess",
    "quadrant": "Tools",
    "isNew": "FALSE",
    "description": "<p><strong><a href=\"https://databrickslabs.github.io/overwatch/\">Databricks Overwatch</a></strong> is a Databricks Labs project that enables teams to analyze various operational metrics of Databricks workloads around cost, governance and performance with support to run what-if experiments. It's essentially a set of data pipelines that populate tables in Databricks, which can then be analyzed using tools like notebooks. Overwatch is very much a power tool; however, it's still in its early stages and it may take some effort to set it up — our use of it required Databricks solution architects to help set it up and populate a price reference table for cost calculations — but we expect adoption to get easier over time. The level of analysis made possible by Overwatch is deeper than what is allowed by cloud providers' cost analysis tools. For example, we were able to analyze the cost of job failures — recognizing that failing fast saves money compared to jobs that only fail near the final step — and break down the cost by various groupings (workspace, cluster, job, notebook, team). We also appreciated the improved operational visibility, as we could easily audit access controls around cluster configurations and analyze operational metrics like finding the longest running notebook or largest read/write volume. Overwatch can analyze historical data, but its real-time mode allows for alerting which helps you to add appropriate controls to your Databricks workloads.</p>"
  },
  {
    "name": "io-ts",
    "ring": "Adopt",
    "quadrant": "Languages & Frameworks",
    "isNew": "TRUE",
    "description": "<p>Our teams developing in <a href=\"/radar/languages-and-frameworks/typescript\">TypeScript</a> are finding <strong><a href=\"https://gcanti.github.io/io-ts/\">io-ts</a></strong> invaluable, especially when interacting with APIs that ultimately result in the creation of objects with specific types. When working with TypeScript, getting data into the bounds of the type system (i.e., from the aforementioned APIs) can lead to run-time errors that can be hard to find and debug. io-ts bridges the gap between compile-time type checking and run-time consumption of external data by providing encode and decode functions. Given the experiences of our teams and the elegance of its approach, we think io-ts is worth adopting.</p>"
  },
  {
    "name": "Carbon",
    "ring": "Hold",
    "quadrant": "Languages & Frameworks",
    "isNew": "FALSE",
    "description": "<p>We're seeing some interest in the <strong><a href=\"https://github.com/carbon-language/carbon-lang\">Carbon</a></strong> programming language. That doesn't come as a surprise: it has Google's backing and is presented as a natural successor to C++. In our opinion C++ can't be replaced fast enough as software engineers have shown, over the past decades, that writing safe and error-free C++ code is extremely difficult and time-consuming. While Carbon is an interesting concept with its focus on migration from C++, without a working compiler, it's clearly a long way from being usable and there are other modern programming languages that are good choices if you want to migrate from C++. It's too early to tell whether Carbon will become the natural successor to C++, but, from today's perspective, we recommend that teams look at <a href=\"/radar/languages-and-frameworks/rust\">Rust</a> and <a href=\"/radar/languages-and-frameworks/go-language\">Go</a> rather than postponing a migration because they're waiting for Carbon to arrive.</p>"
  },
  {
    "name": "Team cognitive load",
    "ring": "Adopt",
    "quadrant": "Techniques",
    "isNew": "TRUE",
    "description": "<p>Team interaction is a key concept when redesigning an organization for business agility and speed. These interactions will be reflected in the software being built (see <a href=\"https://www.thoughtworks.com/about-us/news/2021/latest-thoughtworks-technology-radar-proclaims---embrace-conway-\">Conway's Law</a>) and indicate how effectively teams can autonomously deliver value to their customers. Our advice is to be intentional about how teams are designed and how they interact. Because we believe that organizational design and team interactions evolve over time, we think it's particularly important to measure and keep track of the <strong>team cognitive load</strong>, which indicates how easy or difficult teams find building, testing and maintaining their services. We've been using a <a href=\"https://github.com/TeamTopologies/Team-Cognitive-Load-Assessment\">template</a> to assess team cognitive load that is based on ideas by the authors of the <em><a href=\"https://teamtopologies.com/book\">Team Topologies</a></em> book.</p>"
  },
  {
    "name": "Threat modeling",
    "ring": "Adopt",
    "quadrant": "Techniques",
    "isNew": "FALSE",
    "description": "<p>We continue to recommend that teams carry out <strong><a href=\"https://www.owasp.org/index.php/Category:Threat_Modeling\">threat modeling</a></strong> — a set of techniques to help you identify and classify potential threats during the development process — but we want to emphasize that this is not a one-off activity only done at the start of projects; teams need to avoid the <a href=\"/radar/techniques/security-sandwich\">security sandwich</a>. This is because throughout the lifetime of any software, new threats will emerge and existing ones will continue to evolve thanks to external events and ongoing changes to requirements and architecture. This means that threat modeling needs to be repeated periodically — the frequency of repetition will depend on the circumstances and will need to consider factors such as the cost of running the exercise and the potential risk to the business. When used in conjunction with other techniques, such as establishing cross-functional security requirements to address common risks in the project's technologies and using automated security scanners, threat modeling can be a powerful asset.</p>"
  },
  {
    "name": "Backstage",
    "ring": "Adopt",
    "quadrant": "Techniques",
    "isNew": "FALSE",
    "description": "<p>In an increasingly digital world, improving developer effectiveness in large organizations is often a core concern of senior leaders. We've seen enough value with developer portals in general and <strong><a href=\"https://backstage.io/\">Backstage</a></strong> in particular that we're happy to recommend it in Adopt. Backstage is an open-source developer portal platform created by Spotify that improves discovery of software assets across the organization. It uses Markdown <a href=\"https://backstage.io/docs/features/techdocs/techdocs-overview\">TechDocs</a> that live alongside the code for each service, which nicely balances the needs of centralized discovery with the need for distributed ownership of assets. Backstage supports software templates to accelerate new development and a plugin architecture that allows for extensibility and adaptability into an organization's infrastructure ecosystem. <a href=\"https://backstage.io/docs/features/software-catalog/software-catalog-overview\">Backstage Service Catalog</a> uses YAML files to track ownership and metadata for all the software in an organization's ecosystem; it even lets you track third-party SaaS software, which usually requires tracking ownership.</p>"
  },
  {
    "name": "Delta Lake",
    "ring": "Adopt",
    "quadrant": "Techniques",
    "isNew": "FALSE",
    "description": "<p><strong><a href=\"https://delta.io/\">Delta Lake</a></strong> is an <a href=\"https://github.com/delta-io/delta\">open-source storage layer</a>, implemented by Databricks, that attempts to bring ACID transactions to big data processing. In our Databricks-enabled <a href=\"/radar/techniques/data-lake\">data lake</a> or <a href=\"/radar/techniques/data-mesh\">data mesh</a> projects, our teams prefer using Delta Lake storage over the direct use of file storage types such as <a href=\"https://aws.amazon.com/s3/\">AWS S3</a> or <a href=\"https://azure.microsoft.com/en-au/services/storage/data-lake-storage/\">ADLS</a>. Until recently, Delta Lake has been a closed proprietary product from Databricks, but it's now open source and accessible to non-Databricks platforms. However, our recommendation of Delta Lake as a default choice currently extends only to Databricks projects that use <a href=\"https://parquet.apache.org/\">Parquet</a> file formats. Delta Lake facilitates concurrent data read/write use cases where file-level transactionality is required. We find Delta Lake's seamless integration with Apache Spark <a href=\"https://docs.databricks.com/delta/delta-batch.html\">batch</a> and <a href=\"https://docs.databricks.com/delta/delta-streaming.html\">micro-batch</a> APIs very helpful, particularly features such as <a href=\"https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\">time travel</a> (accessing data at a particular point in time or commit reversion) as well as <a href=\"https://databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html\">schema evolution</a> support on write.</p>"
  },
  {
    "name": "Delta Lake",
    "ring": "Adopt",
    "quadrant": "Techniques",
    "isNew": "FALSE",
    "description": "<p><strong><a href=\"https://delta.io/\">Delta Lake</a></strong> is an <a href=\"https://github.com/delta-io/delta\">open-source storage layer</a>, implemented by Databricks, that attempts to bring ACID transactions to big data processing. In our Databricks-enabled <a href=\"/radar/techniques/data-lake\">data lake</a> or <a href=\"/radar/techniques/data-mesh\">data mesh</a> projects, our teams prefer using Delta Lake storage over the direct use of file storage types such as <a href=\"https://aws.amazon.com/s3/\">AWS S3</a> or <a href=\"https://azure.microsoft.com/en-au/services/storage/data-lake-storage/\">ADLS</a>. Until recently, Delta Lake has been a closed proprietary product from Databricks, but it's now open source and accessible to non-Databricks platforms. However, our recommendation of Delta Lake as a default choice currently extends only to Databricks projects that use <a href=\"https://parquet.apache.org/\">Parquet</a> file formats. Delta Lake facilitates concurrent data read/write use cases where file-level transactionality is required. We find Delta Lake's seamless integration with Apache Spark <a href=\"https://docs.databricks.com/delta/delta-batch.html\">batch</a> and <a href=\"https://docs.databricks.com/delta/delta-streaming.html\">micro-batch</a> APIs very helpful, particularly features such as <a href=\"https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\">time travel</a> (accessing data at a particular point in time or commit reversion) as well as <a href=\"https://databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html\">schema evolution</a> support on write.</p>"
  },
  {
    "name": "Great Expectations",
    "ring": "Adopt",
    "quadrant": "Techniques",
    "isNew": "FALSE",
    "description": "<p><a href=\"https://docs.greatexpectations.io/en/latest/\"><strong>Great Expectations</strong></a> has become a sensible default for our teams in the data quality space, which is why we recommend adopting it — not only for the lack of better alternatives but also because our teams have reported great results in several client projects. Great Expectations is a framework that allows you to craft built-in controls that flag anomalies or quality issues in data pipelines. Just as unit tests run in a build pipeline, Great Expectations makes assertions during the execution of a data pipeline. We like its simplicity and ease of use — the rules stored in JSON can be modified by our data domain experts without necessarily needing data engineering skills.</p>"
  },
  {
    "name": "Kotest",
    "ring": "Adopt",
    "quadrant": "Techniques",
    "isNew": "FALSE",
    "description": "<p><strong><a href=\"https://kotest.io/\">Kotest</a></strong> (previously KotlinTest) is a stand-alone testing tool for the <a href=\"/radar/languages-and-frameworks/kotlin\">Kotlin</a> ecosystem that is widely used among our teams across various Kotlin implementations — native, JVM or JavaScript. Its key advantages are that it offers a variety of testing styles in order to structure test suites and that it comes with a comprehensive set of matchers, which allow for expressive tests in an elegant internal DSL. In addition to its support for <a href=\"/radar/techniques/property-based-unit-testing\">property-based testing</a>, our teams like the solid IntelliJ plugin and the support community. Many of our developers consider it their first choice and recommend those who are still using JUnit in Kotlin consider switching over to Kotest.</p>"
  },
  {
    "name": "NestJS",
    "ring": "Adopt",
    "quadrant": "Techniques",
    "isNew": "FALSE",
    "description": "<p>In the past, we've cautioned about <a href=\"/radar/platforms/node-overload\">Node overload</a>, and we're still cautious about the reasons to choose it. However, in scenarios where Node.js is required to build back-end applications, our teams are reporting that <strong><a href=\"https://nestjs.com/\">NestJS</a></strong> is a suitable option to enable developers to create testable, scalable, loosely coupled and easily maintainable applications in enterprises. NestJS is a <a href=\"/radar/languages-and-frameworks/typescript\">TypeScript</a>-first framework that makes the development of Node.js applications safer and less error-prone. NestJS is opinionated and comes with SOLID principles and an <a href=\"/radar/languages-and-frameworks/angular\">Angular</a>-inspired architecture out of the box.</p>"
  },
  {
    "name": "React Query",
    "ring": "Adopt",
    "quadrant": "Techniques",
    "isNew": "FALSE",
    "description": "<p><a href=\"https://react-query-v3.tanstack.com/\"><strong>React Query</strong></a> is often described as the missing data-fetching library for <a href=\"/radar/languages-and-frameworks/react-js\">React</a>. Fetching, caching, synchronizing and updating server state is a common requirement in many React applications, and although the requirements are well understood, getting the implementation right is notoriously difficult. React Query provides a straightforward solution using hooks. It works hand-in-hand with existing async data-fetching libraries like <a href=\"/radar/tools/axios\">axios</a>, <a href=\"/radar/languages-and-frameworks/fetch\">Fetch</a> and <a href=\"/radar/languages-and-frameworks/graphql\">GraphQL</a> since they are built on promises. As an application developer, you simply pass a function that resolves your data and leave everything else to the framework. We like that it works out of the box but still offers a lot of configuration when needed. The developer tools, unfortunately not yet available for <a href=\"/radar/languages-and-frameworks/react-native\">React Native</a>, also help developers new to the framework understand how it works. For React Native, you can use a <a href=\"https://github.com/bgaleotti/react-query-native-devtools\">third-party developer tools plugin</a> utilizing <a href=\"/radar/tools/flipper\">Flipper</a>. In our experience, version 3 of React Query brought the stability needed to be used in production with our clients.</p>"
  },
  {
    "name": "Swift Package Manager",
    "ring": "Adopt",
    "quadrant": "Techniques",
    "isNew": "FALSE",
    "description": "<p>When introduced in 2014, Swift didn't come with a package manager. Later, <strong><a href=\"https://github.com/apple/swift-package-manager\">Swift Package Manager</a></strong> was created as an official Apple open-source project, and this solution has continued to develop and mature. Our teams rely increasingly on SwiftPM because most packages can be included through it and the processes for both creators and consumers of packages have been streamlined. In the previous Radar, we recommended trialing, but we now believe it makes sense to select it as the default when starting new projects. For existing projects using tools like CocoaPods or <a href=\"/radar/tools/carthage\">Carthage</a>, it might be worth a quick experiment to gauge the level of effort to migrate and to check whether all dependencies are available.</p>"
  }
]
